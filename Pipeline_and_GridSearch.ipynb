{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Pipeline and Grid Search\n",
    "+ Импорт данных\n",
    "+ Преобразование признаков(PolynomialExpansion)\n",
    "+ Масштабирование\n",
    "+ Обучение и применение модели\n",
    "+ Объединение в Pipeline\n",
    "+ Grid Search\n",
    "+ Cross Validation\n",
    "## Импорт данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----------------+\n",
      "|label|         Features|\n",
      "+-----+-----------------+\n",
      "|  0.0|[5.1,3.5,1.4,0.2]|\n",
      "|  0.0|[4.9,3.0,1.4,0.2]|\n",
      "|  0.0|[4.7,3.2,1.3,0.2]|\n",
      "|  0.0|[4.6,3.1,1.5,0.2]|\n",
      "|  0.0|[5.0,3.6,1.4,0.2]|\n",
      "+-----+-----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "data = [label: double, Features: vector]\n",
       "splited = Array([label: double, Features: vector], [label: double, Features: vector])\n",
       "train = [label: double, Features: vector]\n",
       "test = [label: double, Features: vector]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "lastException: Throwable = null\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[label: double, Features: vector]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.mllib.regression.LabeledPoint\n",
    "import org.apache.spark.mllib.linalg.Vectors\n",
    "\n",
    "val data = sqlContext.read.parquet(\"/user/supp.bda08/iris.parquet\").toDF(\"label\",\"Features\")\n",
    "\n",
    "data.show(5)\n",
    "\n",
    "val splited = data.randomSplit(weights = Array(0.85, 0.15), seed = 11)\n",
    "\n",
    "val train = splited(0).cache()\n",
    "val test = splited(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Преобразование признаков\n",
    "Попробуем для начала степенное преобразование признаков 2 степени \n",
    "с помощью трансформера PolynomialExpansion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "polynomialExpansion: org.apache.spark.ml.feature.PolynomialExpansion = poly_8b0f52847fd1\n",
       "polyDF: org.apache.spark.sql.DataFrame = [label: double, Features: vector, polyFeatures: vector]\n",
       "polyDF_test: org.apache.spark.sql.DataFrame = [label: double, Features: vector, polyFeatures: vector]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|        polyFeatures|\n",
      "+--------------------+\n",
      "|[4.3,18.49,3.0,12...|\n",
      "|[4.4,19.360000000...|\n",
      "|[4.4,19.360000000...|\n",
      "|[4.4,19.360000000...|\n",
      "|[4.5,20.25,2.3,10...|\n",
      "+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import org.apache.spark.ml.feature.PolynomialExpansion\n",
    "\n",
    "val polynomialExpansion = new PolynomialExpansion()\n",
    "  .setInputCol(\"Features\")\n",
    "  .setOutputCol(\"polyFeatures\")\n",
    "  .setDegree(2)\n",
    "val polyDF = polynomialExpansion.transform(train)\n",
    "val polyDF_test = polynomialExpansion.transform(test)\n",
    "\n",
    "polyDF.select(\"polyFeatures\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Масштабирование\n",
    "Сделаем масштабирование признаков с помощью StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "scaler: org.apache.spark.ml.feature.StandardScaler = stdScal_1bdced9ed3c7\n",
       "scalerModel: org.apache.spark.ml.feature.StandardScalerModel = stdScal_1bdced9ed3c7\n",
       "transformed: org.apache.spark.sql.DataFrame = [label: double, features: vector, polyFeatures: vector]\n",
       "transformed_test: org.apache.spark.sql.DataFrame = [label: double, features: vector, polyFeatures: vector]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+--------------------+\n",
      "|label|            features|        polyFeatures|\n",
      "+-----+--------------------+--------------------+\n",
      "|  0.0|[5.09330426783669...|[4.3,18.49,3.0,12...|\n",
      "|  0.0|[5.21175320429801...|[4.4,19.360000000...|\n",
      "|  0.0|[5.21175320429801...|[4.4,19.360000000...|\n",
      "|  0.0|[5.21175320429801...|[4.4,19.360000000...|\n",
      "|  0.0|[5.33020214075933...|[4.5,20.25,2.3,10...|\n",
      "+-----+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import org.apache.spark.ml.feature.StandardScaler\n",
    "\n",
    "val scaler = new StandardScaler()\n",
    "  .setInputCol(\"polyFeatures\")\n",
    "  .setOutputCol(\"features\")\n",
    "  .setWithStd(true)\n",
    "  .setWithMean(false)\n",
    "\n",
    "val scalerModel = scaler.fit(polyDF)\n",
    "\n",
    "val transformed = scalerModel.transform(polyDF)\n",
    "val transformed_test = scalerModel.transform(polyDF_test)\n",
    "\n",
    "transformed.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Обучение и применение модели\n",
    "Обучим модель случайного леса с тремя классами. Для этого обернем наш классификатор в модель OneVsRest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "classifier: org.apache.spark.ml.classification.RandomForestClassifier = rfc_b1c47e4c5ac6\n",
       "ovr: org.apache.spark.ml.classification.OneVsRest = oneVsRest_2b2b0c05dcd4\n",
       "model: org.apache.spark.ml.classification.OneVsRestModel = oneVsRest_2b2b0c05dcd4\n",
       "preds: org.apache.spark.sql.DataFrame = [label: double, Features: vector, polyFeatures: vector, features: vector, prediction: double]\n",
       "evaluator: org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator = mcEval_ae2f905a07ae\n",
       "f1_score: Double = 0.8584356819650937\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1 score = 0.8584356819650937\n"
     ]
    }
   ],
   "source": [
    "import org.apache.spark.ml.classification.{OneVsRest, RandomForestClassifier}\n",
    "import org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator\n",
    "\n",
    "val classifier = new RandomForestClassifier()\n",
    "    .setNumTrees(10)\n",
    "val ovr = new OneVsRest()\n",
    "ovr.setClassifier(classifier)\n",
    "\n",
    "val model = ovr.fit(transformed)\n",
    "\n",
    "\n",
    "val preds = model.transform(transformed_test)\n",
    "val evaluator = new MulticlassClassificationEvaluator()\n",
    "val f1_score = evaluator.evaluate(preds)\n",
    "\n",
    "println(s\"f1 score = ${f1_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Pipeline\n",
    "Сложим все эти шаги в единый пайплайн "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "polynomialExpansion: org.apache.spark.ml.feature.PolynomialExpansion = poly_3433f58d3201\n",
       "scaler: org.apache.spark.ml.feature.StandardScaler = stdScal_65fd456627e5\n",
       "classifier: org.apache.spark.ml.classification.RandomForestClassifier = rfc_748a484d64c6\n",
       "ovr: org.apache.spark.ml.classification.OneVsRest = oneVsRest_791cc8b3b8bd\n",
       "pipeline: org.apache.spark.ml.Pipeline = pipeline_0c382ee84476\n",
       "pipelineModel: org.apache.spark.ml.PipelineModel = pipeline_0c382ee84476\n",
       "preds: org.apache.spark.sql.DataFrame = [label: double, Features: vector, polyFeatures: vector, features: vector, prediction: double]\n",
       "evaluator: org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator = mcEval_fed14d6bd7f2\n",
       "f1_score: Double = 0.8584356819650937\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1 score = 0.8584356819650937\n"
     ]
    }
   ],
   "source": [
    "import org.apache.spark.ml.Pipeline\n",
    "\n",
    "val polynomialExpansion = new PolynomialExpansion()\n",
    "  .setInputCol(\"Features\")\n",
    "  .setOutputCol(\"polyFeatures\")\n",
    "\n",
    "val scaler = new StandardScaler()\n",
    "  .setInputCol(\"polyFeatures\")\n",
    "  .setOutputCol(\"features\")\n",
    "  .setWithStd(true)\n",
    "  .setWithMean(false)\n",
    "\n",
    "val classifier = new RandomForestClassifier()\n",
    "val ovr = new OneVsRest()\n",
    "ovr.setClassifier(classifier)\n",
    "\n",
    "\n",
    "\n",
    "val pipeline = new Pipeline()\n",
    "  .setStages(Array(polynomialExpansion, scaler, ovr))\n",
    "val pipelineModel = pipeline.fit(train)\n",
    "\n",
    "\n",
    "val preds = pipelineModel.transform(test)\n",
    "val evaluator = new MulticlassClassificationEvaluator()\n",
    "val f1_score = evaluator.evaluate(preds)\n",
    "\n",
    "println(s\"f1 score = ${f1_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Grid Search\n",
    "Создадим простую сетку параметров:\n",
    "+ Степень полинома (2, 3)\n",
    "+ Количество деревьев в алгоритме случайного леса (5, 10, 20, 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "paramGrid = \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Array({\n",
       "\tpoly_3433f58d3201-degree: 2,\n",
       "\trfc_748a484d64c6-numTrees: 5\n",
       "}, {\n",
       "\tpoly_3433f58d3201-degree: 3,\n",
       "\trfc_748a484d64c6-numTrees: 5\n",
       "}, {\n",
       "\tpoly_3433f58d3201-degree: 2,\n",
       "\trfc_748a484d64c6-numTrees: 10\n",
       "}, {\n",
       "\tpoly_3433f58d3201-degree: 3,\n",
       "\trfc_748a484d64c6-numTrees: 10\n",
       "}, {\n",
       "\tpoly_3433f58d3201-degree: 2,\n",
       "\trfc_748a484d64c6-numTrees: 20\n",
       "}, {\n",
       "\tpoly_3433f58d3201-degree: 3,\n",
       "\trfc_748a484d64c6-numTrees: 20\n",
       "}, {\n",
       "\tpoly_3433f58d3201-degree: 2,\n",
       "\trfc_748a484d64c6-numTrees: 30\n",
       "}, {\n",
       "\tpoly_3433f58d3201-degree: 3,\n",
       "\trfc_748a484d64c6-numTrees: 30\n",
       "})\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[{\n",
       "\tpoly_3433f58d3201-degree: 2,\n",
       "\trfc_748a484d64c6-numTrees: 5\n",
       "}, {\n",
       "\tpoly_3433f58d3201-degree: 3,\n",
       "\trfc_748a484d64c6-numTrees: 5\n",
       "}, {\n",
       "\tpoly_3433f58d3201-degree: 2,\n",
       "\trfc_748a484d64c6-numTrees: 10\n",
       "}, {\n",
       "\tpoly_3433f58d3201-degree: 3,\n",
       "\trfc_748a484d64c6-numTrees: 10\n",
       "}, {\n",
       "\tpoly_3433f58d3201-degree: 2,\n",
       "\trfc_748a484d64c6-numTrees: 20\n",
       "}, {\n",
       "\tpoly_3433f58d3201-degree: 3,\n",
       "\trfc_748a484d64c6-numTrees: 20\n",
       "}, {\n",
       "\tpoly_3433f58d3201-degree: 2,\n",
       "\trfc_748a484d64c6-numTrees: 30\n",
       "}, {\n",
       "\tpoly_3433f58d3201-degree: 3,\n",
       "\trfc_748a484d64c6-numTrees: 30\n",
       "}]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.tuning.{ParamGridBuilder, CrossValidator}\n",
    "\n",
    "val paramGrid = new ParamGridBuilder()\n",
    "  .addGrid(polynomialExpansion.degree, Array(2, 3))\n",
    "  .addGrid(classifier.numTrees, Array(5, 10, 20, 30))\n",
    "  .build()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Cross Validation\n",
    "Создадим модель кросс-валидации по нашему пайплайну, 3 фолдам и сетке параметров.\n",
    "В результате estimator вернет нам лучшую модель по выбранной метрике. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "cv = cv_d683d14201c6\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "cv_d683d14201c6"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val cv = new CrossValidator()\n",
    "  .setEstimator(pipeline)\n",
    "  .setEvaluator(new MulticlassClassificationEvaluator)\n",
    "  .setEstimatorParamMaps(paramGrid)\n",
    "  .setNumFolds(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "model = cv_d683d14201c6\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "cv_d683d14201c6"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val model = cv.fit(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Посмотрим на результаты лучшей модели: метрику и параметры"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "f1 = 0.9056122448979592\n",
       "best_params = \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{\n",
       "\tpoly_3433f58d3201-degree: 2,\n",
       "\trfc_748a484d64c6-numTrees: 30\n",
       "}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val f1 = evaluator.evaluate(model.transform(test))\n",
    "\n",
    "val best_params = model.getEstimatorParamMaps\n",
    "    .zip(model.avgMetrics)\n",
    "    .maxBy(_._2)\n",
    "    ._1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - Scala - DL4j",
   "language": "scala",
   "name": "apache_toree_scala-dl4j"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "2.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
