{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Vowpal wabbit\n",
    "\n",
    "* Spark-Обертка\n",
    "* Обучение\n",
    "* Скоринг"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Spark-обертка\n",
    "https://github.com/JohnLangford/vowpal_wabbit/tree/master/cluster/spark\n",
    "\n",
    "Перед запуском обучения нужно запускать демон spanning_tree (Уже запущен)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined class PipeUtils\n",
       "defined class VwSparkCluster\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import java.net.InetAddress\n",
    "import org.apache.spark.Logging\n",
    "\n",
    "\n",
    "/**\n",
    "  * The goal of this class is to provide as easy method to pipe data through an external command.  It is done by combining\n",
    "  * a {@link PipedOutputStream} with a {@link PipedInputStream} to create a single pipe to feed data through.  This is\n",
    "  * done asynchronously so data can be read and written to at the same time.\n",
    "  * Created by jmorra on 1/22/15.\n",
    "  */\n",
    "class PipeUtils(bufferSize: Int = 1 << 20) extends Serializable{\n",
    "  import java.io._\n",
    "\n",
    "  import scala.concurrent.ExecutionContext.Implicits.global\n",
    "  import scala.concurrent.Future\n",
    "  import scala.language.postfixOps\n",
    "  import scala.sys.process._\n",
    "\n",
    "  /**\n",
    "    * This implicit class will allow easy access to streaming through external processes.  This\n",
    "    * should work on a line by line basis just like Spark's pipe command.\n",
    "    * http://stackoverflow.com/questions/28095469/stream-input-to-external-process-in-scala\n",
    "    * @param s: The input stream\n",
    "    */\n",
    "  implicit class IteratorStream(s: TraversableOnce[String]) {\n",
    "    def pipe(cmd: String): Stream[String] = cmd #< iter2is(s) lines\n",
    "    def pipe(cmd: Seq[String]): Stream[String] = cmd #< iter2is(s) lines\n",
    "    def run(cmd: String): String = cmd #< iter2is(s) !!\n",
    "\n",
    "    private[this] def iter2is[A](it: TraversableOnce[A]): InputStream = {\n",
    "      // What is written to the output stream will appear in the input stream.\n",
    "      val pos = new PipedOutputStream\n",
    "\n",
    "      val pis = new PipedInputStream(pos, bufferSize)\n",
    "      val w = new PrintWriter(new BufferedOutputStream(pos, bufferSize), false)\n",
    "\n",
    "      // Scala 2.11 (scala 2.10, use 'future').  Executes asynchronously.\n",
    "      // Fill the stream, then close.\n",
    "      Future {\n",
    "        try it.foreach(w.println)\n",
    "        finally w.close()\n",
    "      }\n",
    "\n",
    "      // Return possibly before pis is fully written to.\n",
    "      pis\n",
    "    }\n",
    "  }\n",
    "}\n",
    "\n",
    "/**\n",
    "  * A framework for running VW in a cluster environment using <a href=\"http://spark.apache.org/\">Apache Spark</a>.  This\n",
    "  * is meant only as a framework and may require some modification to work under your specific case.\n",
    "  * Created by jmorra on 8/19/15.\n",
    "  */\n",
    "case class VwSparkCluster( pipeUtils: PipeUtils = new PipeUtils,\n",
    "                           ipAddress: String = InetAddress.getLocalHost.getHostAddress,\n",
    "                           defaultParallelism: Int = 2) extends Logging {\n",
    "\n",
    "  import java.io._\n",
    "  import org.apache.commons.io.IOUtils\n",
    "  import org.apache.spark.rdd.RDD\n",
    "  import org.apache.spark.SparkContext\n",
    "  import scala.sys.process._\n",
    "  import pipeUtils._\n",
    "\n",
    "  /**\n",
    "    * This will learn a VW model in cluster mode.  If you notice that this command never starts and just stalls then the parallelism\n",
    "    * is probably too high.  Refer to <a href=\"https://github.com/JohnLangford/vowpal_wabbit/wiki/Cluster_parallel.pdf\">this</a>\n",
    "    * for more information.\n",
    "    * @param data an RDD of Strings that are in VW input format.\n",
    "    * @param vwCmd the VW command to run.  Note that this command must NOT contain --cache_file and -f.  Those will automatically\n",
    "    *              be appended if necessary.\n",
    "    * @param parallelism the amount of parallelism to use.  This is calculated using a formula defined in getParallelism\n",
    "    *                    if it is not supplied.  It is recommended to only supply this if getParallelism is not working\n",
    "    *                    in you case.\n",
    "    * @return a byte array containing the final VW model.\n",
    "    */\n",
    "  def train(data: RDD[String], vwCmd: String, parallelism: Option[Int] = None): Array[Byte] = {\n",
    "    if (numberOfRunningProcesses(\"spanning_tree\") != 1) {\n",
    "      throw new IllegalStateException(\"spanning_tree is not running on the driver, cannot proceed.  Please start spanning_tree and try again.\")\n",
    "    }\n",
    "\n",
    "    val sc = data.context\n",
    "    val conf = sc.getConf\n",
    "\n",
    "    // By using the job id and the RDD id we should get a globally unique ID.\n",
    "    val jobId = (conf.get(\"spark.app.id\").replaceAll(\"[^\\\\d]\", \"\") + data.id).toLong\n",
    "    logInfo(s\"VW cluster job ID: $jobId\")\n",
    "\n",
    "    val partitions = parallelism.getOrElse(getParallelism(sc).getOrElse(defaultParallelism))\n",
    "    logInfo(s\"VW cluster parallelism: ${partitions}\")\n",
    "\n",
    "    val repartitionedData = if (data.partitions.length == partitions) data else data.repartition(partitions)\n",
    "\n",
    "    val vwBaseCmd = s\"$vwCmd --total $partitions --span_server $ipAddress --unique_id $jobId\"\n",
    "    logInfo(s\"VW cluster baseCmd: $vwBaseCmd\")\n",
    "\n",
    "    val vwModels = repartitionedData.mapPartitionsWithIndex {case (partition, x) =>\n",
    "      Iterator(runVWOnPartition(vwBaseCmd, x, partition))\n",
    "    }\n",
    "\n",
    "    vwModels.collect.flatten.flatten\n",
    "  }\n",
    "\n",
    "  def numberOfRunningProcesses(process: String): Int = \"ps aux\".#|(s\"grep $process\").!!.split(\"\\n\").length - 1\n",
    "\n",
    "  /**\n",
    "    * Gets the executor storage status excluding the driver node.\n",
    "    * @param sc the SparkContext\n",
    "    * @return an Array of Strings that are the names of all the storage statuses.\n",
    "    */\n",
    "  def executors(sc: SparkContext): Array[String] = {\n",
    "    sc.getExecutorStorageStatus.collect{\n",
    "      case x if x.blockManagerId.executorId != \"<driver>\" =>\n",
    "        x.blockManagerId.executorId\n",
    "    }\n",
    "  }\n",
    "\n",
    "  /**\n",
    "    * Gets the parallelism of the cluster.  This is very much so a work in progress that seems to work now.  This took\n",
    "    * a lot of experimentation on Spark 1.2.0 to get to work.  I make no guarantees that it will work on other Spark versions\n",
    "    * especially if <a href=\"https://spark.apache.org/docs/1.2.0/job-scheduling.html#dynamic-resource-allocation\">dynamic\n",
    "    * allocation</a> is enabled.  I also only tested this with a master of yarn-client and local so I'm not sure how\n",
    "    * well it'll behave in other resource management environments (Spark Standalone, Mesos, etc.).\n",
    "    * @param sc the SparkContext\n",
    "    * @return if the parallelism can be found then the expected amount of parallelism.\n",
    "    */\n",
    "  def getParallelism(sc: SparkContext): Option[Int] = {\n",
    "    sc.master match {\n",
    "      case x if x.contains(\"yarn\") => sc.getConf.getOption(\"spark.executor.cores\").map(x => x.toInt * executors(sc).length)\n",
    "      case _ => Some(sc.defaultParallelism)\n",
    "    }\n",
    "  }\n",
    "\n",
    "  /**\n",
    "    * This will accept a base VW command, and append a cache file if necessary.  It will also create a temp file\n",
    "    * to store the VW model.  It will then run VW on the supplied data.  Finally it will return the bytes of the\n",
    "    * model ONLY if the partition is 0.\n",
    "    *\n",
    "    * This function was tricky to write because the end result of each calculation is a file on the local disk.\n",
    "    * According to John all the models should be in the same state after learning so we can choose to save\n",
    "    * anyone we want, therefore, transferring the contents of each file to the driver would be wasteful.\n",
    "    * In order to avoid this unnecessary transfer we're just going to get the first file.  Now you might\n",
    "    * ask yourself why not just call .first on the RDD.  We cannot do that because in that case Spark would\n",
    "    * only evaluate the first mapper and we need all of them to be evaluated, hence the need for .collect to\n",
    "    * be called.  Note that you may have to increase spark.driver.maxResultSize if the size of the VW model\n",
    "    * is too large.\n",
    "    * @param vwBaseCmd the base VW command without a cache file or an output specified.  A cache file will automatically\n",
    "    *                  be used if --passes is specified.\n",
    "    * @param data a String a data in VW format to be passed to VW\n",
    "    * @param partition the partition number of this chunk of data\n",
    "    * @return an Array of the bytes of the VW model ONLY if this is the 0th partition, else None.\n",
    "    */\n",
    "  def runVWOnPartition(vwBaseCmd: String, data: Iterator[String], partition: Int): Option[Array[Byte]] = {\n",
    "    val cacheFile = if (vwBaseCmd.contains(\"--passes \")) {\n",
    "      val c = File.createTempFile(\"vw-cache\", \".cache\")\n",
    "      c.deleteOnExit()\n",
    "      Option(c)\n",
    "    } else None\n",
    "    val vwBaseCmdWithCache = cacheFile.map(x => s\"$vwBaseCmd -k --cache_file ${x.getCanonicalPath}\").getOrElse(vwBaseCmd)\n",
    "\n",
    "    val output = File.createTempFile(\"vw-model\", \".model\")\n",
    "    output.deleteOnExit()\n",
    "    val vwCmd = s\"$vwBaseCmdWithCache --node $partition -f ${output.getCanonicalPath}\"\n",
    "    data.pipe(vwCmd)\n",
    "    cacheFile.foreach(_.delete)\n",
    "\n",
    "    val vwModel = if (partition == 0) {\n",
    "      val inputStream = new BufferedInputStream(new FileInputStream(output))\n",
    "      val byteArray = IOUtils.toByteArray(inputStream)\n",
    "      inputStream.close()\n",
    "      Option(byteArray)\n",
    "    }\n",
    "    else None\n",
    "\n",
    "    output.delete()\n",
    "    vwModel\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Обучение\n",
    "\n",
    "* Создаем объект VwSparkCluster с параметрами по умолчанию.\n",
    "* Считываем данные в RDD[String] в формате vowpal wabbit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Waiting for a Spark session to start..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "cluster: VwSparkCluster = VwSparkCluster($iwC$$iwC$$iwC$$iwC$PipeUtils@2b53a9fe,10.12.49.7,2)\n",
       "data: org.apache.spark.rdd.RDD[String] = /user/supp.bda08/vw.txt MapPartitionsRDD[1] at textFile at <console>:34\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 | price:.23 sqft:.25 age:.05 2006\n",
      "1 2 'second_house | price:.18 sqft:.15 age:.35 1976\n",
      "0 1 0.5 'third_house | price:.53 sqft:.32 age:.87 1924\n"
     ]
    }
   ],
   "source": [
    "val cluster = new VwSparkCluster()\n",
    "val data = sc.textFile(\"/user/supp.bda08/vw.txt\")\n",
    "\n",
    "data.collect.foreach(println)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "* Начинаем обучение передав методу train объекта cluster данные(RDD[String]) и команду vw\n",
    "* Возвращается ByteArray с информацией о модели "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "byteArray = Array(6, 0, 0, 0, 56, 46, 52, 46, 48, 0, 1, 0, 0, 0, 0, 109, 0, 0, 0, 0, 0, 0, -128, 63, 18, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 4, 0, 0, 0, 73, -114, -87, -57, 0, -72, 7, 0, 0, -4, -110, -91, 62, -42, 7, 0, 0, -104, -59, -58, -66, 92, -59, 1, 0, 92, -16, 55, 62, 37, 124, 2, 0, -96, 17, 53, 62, 81, -123, 2, 0, -48, -68, -116, 63, 14, -126, 3, 0, 73, -109, -13, 62)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[6, 0, 0, 0, 56, 46, 52, 46, 48, 0, 1, 0, 0, 0, 0, 109, 0, 0, 0, 0, 0, 0, -128, 63, 18, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 4, 0, 0, 0, 73, -114, -87, -57, 0, -72, 7, 0, 0, -4, -110, -91, 62, -42, 7, 0, 0, -104, -59, -58, -66, 92, -59, 1, 0, 92, -16, 55, 62, 37, 124, 2, 0, -96, 17, 53, 62, 81, -123, 2, 0, -48, -68, -116, 63, 14, -126, 3, 0, 73, -109, -13, 62]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val byteArray = cluster.train(data, \"vw -l 10 -c --passes 25 --holdout_off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Скоринг\n",
    "\n",
    "byteArrayToModel - функция, преобразующая byteArray в файл с моделью в tmp директории"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "byteArrayToModel: (byteArray: Array[Byte])java.io.File\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import java.io.File\n",
    "\n",
    "def byteArrayToModel(byteArray: Array[Byte]): File = {\n",
    "    import java.io._\n",
    "    import java.lang.System\n",
    "    \n",
    "    val tmpDirectoryOp = System.getProperty(\"java.io.tmpdir\")\n",
    "    val tmpDirectory = new File(tmpDirectoryOp)\n",
    "    val fstream = File.createTempFile(\"tempFile\",\".model\", tmpDirectory)\n",
    "    \n",
    "    val bos = new BufferedOutputStream(new FileOutputStream(fstream))\n",
    "    Stream.continually(bos.write(byteArray))\n",
    "    bos.close()\n",
    "    \n",
    "    fstream\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "##### Локальный скоринг"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "modelFile = /tmp/tempFile9111867150958417472.model\n",
       "result = \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "warning: there were 1 feature warning(s); re-run with -feature for details\n",
       "\"0.000002\n",
       "\"\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.000002\n",
       "1.000000 second_house\n",
       "1 third_house\n"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys.process._\n",
    "val modelFile: File = byteArrayToModel(byteArray)\n",
    "val result = s\"vw -i ${modelFile.getPath} -t vw.txt -p /dev/stdout --quiet\" !!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "##### Распределенный скоринг на RDD[String]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "resultDistributed = MapPartitionsRDD[3] at mapPartitions at <console>:47\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "warning: there were 1 feature warning(s); re-run with -feature for details\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "MapPartitionsRDD[3] at mapPartitions at <console>:47"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val resultDistributed = data.mapPartitions(iter => {\n",
    "    val modelFile = byteArrayToModel(byteArray)\n",
    "    val res = iter.map(line => s\"echo $line\" #| s\"vw -i ${modelFile.getPath} -p /dev/stdout --quiet\" !!)\n",
    "    modelFile.deleteOnExit()\n",
    "    \n",
    "    res\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 | price:.23 sqft:.25 age:.05 2006\n",
      "1 2 'second_house | price:.18 sqft:.15 age:.35 1976\n",
      "0 1 0.5 'third_house | price:.53 sqft:.32 age:.87 1924\n"
     ]
    }
   ],
   "source": [
    "data.collect.foreach(println)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.000002\n",
      "1.000000 second_house\n",
      "1 third_house\n"
     ]
    }
   ],
   "source": [
    "resultDistributed.collect.foreach(print)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - Scala",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "2.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
